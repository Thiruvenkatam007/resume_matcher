{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3376de0",
   "metadata": {},
   "source": [
    "# Normal implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b437018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# LangChain (modern imports)\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "import backoff\n",
    "import numpy as np\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# ========== Logging ==========\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\"\n",
    ")\n",
    "\n",
    "# ========== IO helpers ==========\n",
    "\n",
    "def load_yaml_job_description(path: str) -> Dict[str, Any]:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        jd = yaml.safe_load(f) or {}\n",
    "    return jd\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "def _read_pdf_pdfminer_md(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract PDF text as Markdown using pdfminer.six.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from pdfminer.high_level import extract_text\n",
    "        text = extract_text(file_path) or \"\"\n",
    "        if text.strip():\n",
    "            # Convert to markdown-friendly (basic)\n",
    "            return \"\\n\".join([line.strip() for line in text.splitlines() if line.strip()])\n",
    "        return \"\"\n",
    "    except Exception as e:  # pragma: no cover\n",
    "        logging.warning(f\"pdfminer failed on {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def _read_pdf_pypdf_md(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract PDF text as Markdown fallback using pypdf.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from pypdf import PdfReader\n",
    "    except Exception:  # pragma: no cover\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        text = []\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            reader = PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                try:\n",
    "                    page_text = page.extract_text() or \"\"\n",
    "                    if page_text.strip():\n",
    "                        # Markdown-friendly formatting\n",
    "                        text.append(\"\\n\".join([ln.strip() for ln in page_text.splitlines() if ln.strip()]))\n",
    "                except Exception:\n",
    "                    text.append(\"\")\n",
    "        return \"\\n\".join(text)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"pypdf failed on {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def load_resume_markdown(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Load resume and return Markdown text.\n",
    "    Priority: pdfminer -> pypdf -> warn if fails.\n",
    "    \"\"\"\n",
    "    file_path = str(file_path)\n",
    "    if file_path.lower().endswith('.txt'):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Could not read TXT {file_path}: {e}\")\n",
    "            return \"\"\n",
    "    elif file_path.lower().endswith('.pdf'):\n",
    "        text = _read_pdf_pdfminer_md(file_path)\n",
    "        if not text.strip():\n",
    "            text = _read_pdf_pypdf_md(file_path)\n",
    "        if not text.strip():\n",
    "            logging.warning(f\"Could not extract text from PDF {file_path}\")\n",
    "        return text\n",
    "    else:\n",
    "        logging.warning(f\"Unsupported resume format: {file_path}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "\n",
    "def safe_candidate_name_from_file(file_name: str) -> str:\n",
    "    stem = Path(file_name).stem\n",
    "    return re.sub(r\"[^A-Za-z0-9_.-]\", \"_\", stem)\n",
    "\n",
    "\n",
    "# ========== Pydantic schema for structured output ==========\n",
    "\n",
    "class MatchReport(BaseModel):\n",
    "    matched_required_skills: List[str] = []\n",
    "    missing_required_skills: List[str] = []\n",
    "    matched_optional_skills: List[str] = []\n",
    "    education_match: str\n",
    "    experience_match: str\n",
    "    keywords_matched: List[str] = []\n",
    "    soft_skills_match: List[str] = []\n",
    "    resume_summary: str\n",
    "    match_score: float = Field(ge=0, le=1)\n",
    "    city_tier_match: bool\n",
    "    longest_tenure_months: int\n",
    "    final_score: int = Field(ge=0, le=100)\n",
    "    # --- extended, optional fields for evaluation criteria ---\n",
    "    detected_city: Optional[str] = None\n",
    "    detected_city_tier: Optional[int] = None  # 1/2/3 if the model can infer\n",
    "    max_job_gap_months: Optional[int] = None\n",
    "    stability_score: Optional[float] = Field(default=None, ge=0, le=1)\n",
    "\n",
    "# ========== Prompt ==========\n",
    "\n",
    "# PROMPT = ChatPromptTemplate.from_messages([\n",
    "#     (\n",
    "#         \"system\",\n",
    "#         \"You are an expert technical recruiter and data scientist. \"\n",
    "#         \"Your job is to read a job description (JD) and a resume chunk, then return a STRICT JSON object matching the schema. \"\n",
    "#         \"Be precise, consistent, and terse. If the information is not present, return a sensible null/empty value rather than guessing. \"\n",
    "#         \"NEVER add commentary, markdown, or keys not in the schema.\"\n",
    "#     ),\n",
    "#     (\n",
    "#         \"user\",\n",
    "#         \"<OBJECTIVE>\\n\"\n",
    "#         \"Evaluate the resume against the JD and produce high-quality, schema-valid JSON capturing skills, education, experience fit, city-tier & gaps, longest tenure, and a calibrated final_score.\\n\\n\"\n",
    "#         \"<INPUTS>\\n\"\n",
    "#         \"Job Description (YAML):\\n{job_description}\\n\\n\"\n",
    "#         \"Resume chunk:\\n{resume_text}\\n\\n\"\n",
    "#         \"<SCHEMA AND CONSTRAINTS>\\n\"\n",
    "#         \"You must return a single JSON object with the following keys and constraints:\\n\"\n",
    "#         \"- matched_required_skills: string[] (subset of JD.required_skills that appear in the resume; normalize case and aliases like js->javascript, py->python, torch->pytorch)\\n\"\n",
    "#         \"- missing_required_skills: string[] (skills from JD.required_skills not evidenced in the resume chunk)\\n\"\n",
    "#         \"- matched_optional_skills: string[] (subset of JD.optional_skills found)\\n\"\n",
    "#         \"- education_match: string (short justification or 'false' if not met; keep to <= 1 sentence)\\n\"\n",
    "#         \"- experience_match: string (short justification or 'false' if not met; <= 1 sentence)\\n\"\n",
    "#         \"- keywords_matched: string[] (notable JD keywords present in resume text)\\n\"\n",
    "#         \"- soft_skills_match: string[] (soft skills evidenced in text; e.g., communication, leadership)\\n\"\n",
    "#         \"- resume_summary: string (1–2 sentences summarizing the candidate relevant to the JD)\\n\"\n",
    "#         \"- match_score: number in [0,1] (your calibrated similarity for THIS CHUNK only)\\n\"\n",
    "#         \"- city_tier_match: boolean (true if city_tier meets or exceeds JD requirement if any; else false)\\n\"\n",
    "#         \"- longest_tenure_months: integer >= 0 (best estimate from dates in this chunk)\\n\"\n",
    "#         \"- final_score: integer in [0,100] (overall score for the FULL candidate, using rubric below; be conservative if context is incomplete)\\n\"\n",
    "#         \"- detected_city: string|null\\n\"\n",
    "#         \"- detected_city_tier: 1|2|3|null\\n\"\n",
    "#         \"- max_job_gap_months: integer|null\\n\"\n",
    "#         \"- stability_score: number in [0,1]|null (optional stability proxy)\\n\\n\"\n",
    "#         \"<EVIDENCE RULES>\\n\"\n",
    "#         \"- Treat the resume chunk as ground truth. Do not infer unstated skills.\\n\"\n",
    "#         \"- Consider common aliases: js↔javascript, ts↔typescript, py↔python, torch↔pytorch, tf↔tensorflow, np↔numpy, sk↔scikit-learn. Normalize to canonical names.\\n\"\n",
    "#         \"- Date parsing: recognize ranges like 'Jan 2019 - Mar 2022', '2018–2021', '2020 to Present'. Compute tenure in months (approx). Present/current = current month. If ambiguous, be conservative.\\n\"\n",
    "#         \"- City & tier: if JD provides a city_tier map, use it; otherwise infer only if city is explicit. If unknown, set both fields null.\\n\\n\"\n",
    "#         \"<RUBRIC FOR final_score (100-point scale)>\\n\"\n",
    "#         \"Use JD-provided weights if present (JD.weights). Otherwise, default weights:\\n\"\n",
    "#         \"- required skills coverage: 40%\\n\"\n",
    "#         \"- optional skills coverage: 15%\\n\"\n",
    "#         \"- experience fit (years/recency/scope): 15%\\n\"\n",
    "#         \"- education fit: 10%\\n\"\n",
    "#         \"- location fit: 5% (true if city_tier meets JD or is unspecified)\\n\"\n",
    "#         \"- stability: 10% (longest_tenure_months; full credit at 48 months; scale proportionally)\\n\"\n",
    "#         \"- diversity by city tier: 5% bonus (Tier-3 > Tier-2 > Tier-1; score 100 for T3, 60 for T2, 0 for T1)\\n\"\n",
    "#         \"Apply a mild penalty for large job gaps via the chunk-level estimate: reduce the total by ~0% (<=3m), 10% (<=6m), 25% (<=12m), 50% (>12m).\\n\"\n",
    "#         \"If JD specifies custom weights/thresholds, follow them exactly.\\n\\n\"\n",
    "#         \"<ROBUSTNESS & STYLE>\\n\"\n",
    "#         \"- Keep outputs concise; arrays deduplicated and normalized to lowercase where appropriate.\\n\"\n",
    "#         \"- If data is missing in this chunk, leave fields empty/null rather than hallucinating.\\n\"\n",
    "#         \"- Never include markdown or commentary—only the JSON object.\\n\\n\"\n",
    "#         \"<OUTPUT> Return ONLY the JSON object.\"\n",
    "#     )\n",
    "# ])\n",
    "\n",
    "PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "    \"system\",\n",
    "    \"You are an expert technical recruiter and data scientist. \"\n",
    "    \"Your job is to read a job description (JD) and a resume , then return a STRICT JSON object matching the schema. \"\n",
    "    \"Be precise, consistent, and terse. If the information is not present, return a sensible null/empty value rather than guessing. \"\n",
    "    \"NEVER add commentary, markdown, or keys not in the schema.\"\n",
    "),\n",
    "(\n",
    "    \"user\",\n",
    "    \"<OBJECTIVE>\\n\"\n",
    "    \"Evaluate the resume against the JD and produce high-quality, schema-valid JSON capturing skills, education, experience fit, city-tier & gaps, longest tenure, and a calibrated final_score.\\n\"\n",
    "    \"\\n\"\n",
    "    \"<INPUTS>\\n\"\n",
    "    \"Job Description (YAML): {job_description}\\n\"\n",
    "    \"Resume : {resume_text}\\n\"\n",
    "    \"\\n\"\n",
    "     \"<RUBRIC FOR final_score (100-point scale)>\\n\"\n",
    "    \"Weightage:\\n\"\n",
    "    \"- required skills coverage: 40%\\n\"\n",
    "    \"- optional skills coverage: 15%\\n\"\n",
    "    \"- experience fit (years/recency/scope): 15%\\n\"\n",
    "    \"- education fit: 10%\\n\"\n",
    "    \"- location fit: 5% (true if city_tier meets JD or is unspecified)\\n\"\n",
    "    \"- stability: 10% (longest_tenure_months; full credit at 48 months; scale proportionally)\\n\"\n",
    "    \"- diversity by city tier: 5% bonus (Tier-3 > Tier-2 > Tier-1; score 100 for T3, 60 for T2, 0 for T1)\\n\"\n",
    "\n",
    "    \"\\n\"\n",
    "    \"<SCHEMA AND CONSTRAINTS>\\n\"\n",
    "    \"You must return a single JSON object with the following keys and constraints: Strictly Don't add any extra key value pair:\\n\"\n",
    "    \"- matched_required_skills: string[] (subset of JD.required_skills that appear in the resume; normalize case and aliases like js->javascript, py->python, torch->pytorch)\\n\"\n",
    "    \"- missing_required_skills: string[] (skills from JD.required_skills not evidenced in the resume chunk)\\n\"\n",
    "    \"- matched_optional_skills: string[] (subset of JD.optional_skills found)\\n\"\n",
    "    \"- education_match: string (short justification or 'false' if not met; keep to <= 1 sentence)\\n\"\n",
    "    \"- experience_match: string (short justification or 'false' if not met; <= 1 sentence)\\n\"\n",
    "    \"- keywords_matched: string[] (notable JD keywords present in resume text)\\n\"\n",
    "    \"- soft_skills_match: string[] (soft skills evidenced in text; e.g., communication, leadership)\\n\"\n",
    "    \"- resume_summary: string (1–2 sentences summarizing the candidate relevant to the JD)\\n\"\n",
    "    \"- match_score: number in [0,1] (your calibrated similarity for THIS resume)\\n\"\n",
    "    \"- city_tier_match: boolean (true if city_tier meets or exceeds JD requirement if any; else false)\\n\"\n",
    "    \"- longest_tenure_months: integer >= 0 (The longest duration (in months) the candidate was employed in a **single company** based on their work history)\\n\"\n",
    "    \"- final_score: integer in [0,100] (An integer between 0 and 100 summarizing the overall resume match quality against the job description based on all criteria above; be conservative if context is incomplete)\\n\"\n",
    "    \"- detected_city: string|null\\n\"\n",
    "    \"- detected_city_tier: 1|2|3|null\\n\"\n",
    "    \"- city_tier_match: boolean (true if detected_city_tier meets the Job description requirement any; else false)\\n\"\n",
    "    \"- max_job_gap_months: integer|null (largest gap in months between consecutive jobs for this resume; compute as difference between next job start and previous job end)\"\n",
    "    \"- max_job_gap_months_check: string|null (short justification if the candidate’s maximum job gap in months is <= Maximum_Job_Gap_Months specified in the Job Description; else null)\"\n",
    "    \"\\n\"\n",
    "    \"<EVIDENCE RULES>\\n\"\n",
    "    \"- Consider common aliases: js↔javascript, ts↔typescript, py↔python, torch↔pytorch, tf↔tensorflow, np↔numpy, sk↔scikit-learn. Normalize to canonical names.\\n\"\n",
    "    \"- Date parsing: recognize ranges like 'Jan 2019 - Mar 2022', '2018–2021', '2020 to Present'. Compute tenure in months (approx). Present/current = current month. If ambiguous, be conservative.\\n\"\n",
    "    \"\\n\"\n",
    "   \n",
    "    \"<ROBUSTNESS & STYLE>\\n\"\n",
    "    \"- Keep outputs concise; arrays deduplicated and normalized to lowercase where appropriate.\\n\"\n",
    "    \"- Never include markdown or commentary—only the JSON object.\\n\"\n",
    "    \"\\n\"\n",
    "    \"<OUTPUT> Return a **single valid JSON object** using this structure(without extra comments or explanations)\"\n",
    ")\n",
    "])\n",
    "\n",
    "# ========== Embeddings (pre-filter) ==========\n",
    "\n",
    "def _cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    if a is None or b is None:\n",
    "        return -1.0\n",
    "    na = np.linalg.norm(a); nb = np.linalg.norm(b)\n",
    "    if na == 0 or nb == 0:\n",
    "        return -1.0\n",
    "    return float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "_embedder: Optional[SentenceTransformerEmbeddings] = None\n",
    "\n",
    "def get_embedder() -> SentenceTransformerEmbeddings:\n",
    "    global _embedder\n",
    "    if _embedder is None:\n",
    "        _embedder = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    return _embedder\n",
    "\n",
    "def embed_text(text: str) -> np.ndarray:\n",
    "    emb = get_embedder().embed_query(text or \"\")\n",
    "    return np.array(emb, dtype=np.float32)\n",
    "\n",
    "\n",
    "def prefilter_resumes(jd: Dict[str, Any], resume_paths: List[Path], texts: List[str], topk: Optional[int] = None, topk_frac: float = 0.4) -> List[Tuple[Path, float]]:\n",
    "    \"\"\"Rank resumes by embedding similarity to the JD and return the top subset.\n",
    "    If topk is None, select ceil(len(resumes) * topk_frac). Never fewer than 1.\n",
    "    \"\"\"\n",
    "    jd_text = yaml.dump(jd, sort_keys=False)\n",
    "    jd_vec = embed_text(jd_text)\n",
    "\n",
    "    sims: List[Tuple[int, float]] = []\n",
    "    for i, t in enumerate(texts):\n",
    "        try:\n",
    "            v = embed_text(t)\n",
    "            sims.append((i, _cosine(jd_vec, v)))\n",
    "        except Exception:\n",
    "            sims.append((i, -1.0))\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    n = len(resume_paths)\n",
    "    k = int(topk) if topk is not None else int(np.ceil(max(1, n) * float(topk_frac)))\n",
    "    k = max(1, min(n, k))\n",
    "\n",
    "    selected = [(resume_paths[i], score) for i, score in sims[:k]]\n",
    "    return selected\n",
    "\n",
    "# ========== LLM client ==========\n",
    "key = os.getenv(\"OPENROUTER_API_KEY\", \"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\")\n",
    "base = os.getenv(\"OPENROUTER_BASE\", \"https://openrouter.ai/api/v1\")\n",
    "# model = os.getenv(\"MODEL_NAME\", \"nvidia/nemotron-nano-9b-v2:free\")\n",
    "model_name = \"qwen/qwen2.5-vl-72b-instruct:free\"\n",
    "\n",
    "def make_llm(model: str = \"gpt-4o-mini\", temperature: float = 0.6):\n",
    "    # print(model_name)\n",
    "    llm = ChatOpenAI(model=model_name, temperature=temperature, api_key=key, base_url=base)\n",
    "    # return llm.with_structured_output(MatchReport)\n",
    "    return llm\n",
    "\n",
    "# ========== Retry wrapper ==========\n",
    "\n",
    "@backoff.on_exception(backoff.expo, Exception, max_time=90)\n",
    "def call_llm_structured(structured_llm, jd_dict: Dict[str, Any], resume_text: str) -> MatchReport:\n",
    "    msg = PROMPT.format(job_description=yaml.dump(jd_dict, sort_keys=False),\n",
    "                        resume_text=resume_text)\n",
    "    \n",
    "    # IMPORTANT: for structured outputs, invoke with messages\n",
    "    return structured_llm.invoke(msg)\n",
    "# ========== Per-resume processing ==========\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "def clean_text_v2(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the first valid JSON object/array from arbitrary text.\n",
    "\n",
    "    - Handles Markdown fences like ```json ... ``` or plain ``` ... ```.\n",
    "    - Ignores any prose before/after the JSON.\n",
    "    - Returns the JSON substring (not a Python dict). If nothing parses,\n",
    "      returns a best-effort substring starting at the first '{' or '['.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    s = text.strip().replace(\"\\u00A0\", \" \")  # normalize non-breaking spaces\n",
    "\n",
    "    # 1) Collect candidates from fenced code blocks (prefer these first).\n",
    "    fence_re = re.compile(r\"```(?:\\w+)?\\s*([\\s\\S]*?)\\s*```\", re.IGNORECASE)\n",
    "    candidates = [m.group(1).strip() for m in fence_re.finditer(s)]\n",
    "\n",
    "    # 2) Also consider the full text in case JSON isn't fenced.\n",
    "    candidates.append(s)\n",
    "\n",
    "    def balanced_json_substrings(src: str):\n",
    "        \"\"\"Yield substrings that are balanced JSON blocks starting at '{' or '['.\"\"\"\n",
    "        out = []\n",
    "        i, n = 0, len(src)\n",
    "        while i < n:\n",
    "            ch = src[i]\n",
    "            if ch in \"{[\":\n",
    "                start = i\n",
    "                stack = [ch]\n",
    "                i += 1\n",
    "                in_str = False\n",
    "                esc = False\n",
    "                while i < n:\n",
    "                    c = src[i]\n",
    "                    if in_str:\n",
    "                        if esc:\n",
    "                            esc = False\n",
    "                        elif c == \"\\\\\":\n",
    "                            esc = True\n",
    "                        elif c == '\"':\n",
    "                            in_str = False\n",
    "                        i += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        if c == '\"':\n",
    "                            in_str = True\n",
    "                        elif c in \"{[\":\n",
    "                            stack.append(c)\n",
    "                        elif c in \"}]\":\n",
    "                            if not stack:\n",
    "                                break\n",
    "                            opening = stack.pop()\n",
    "                            if (opening == \"{\" and c != \"}\") or (opening == \"[\" and c != \"]\"):\n",
    "                                break\n",
    "                            if not stack:\n",
    "                                # Found a balanced block\n",
    "                                out.append(src[start:i+1].strip())\n",
    "                                break\n",
    "                        i += 1\n",
    "                # Move forward to search for the next block\n",
    "                i = start + 1\n",
    "            else:\n",
    "                i += 1\n",
    "        return out\n",
    "\n",
    "    # 3) Try to find a substring that actually parses as JSON.\n",
    "    for cand in candidates:\n",
    "        for sub in balanced_json_substrings(cand):\n",
    "            try:\n",
    "                json.loads(sub)\n",
    "                return sub\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    # 4) Fallback: strip fences and return from the first '{' or '[' onward.\n",
    "    def _strip_fences(m):  # keep inner content\n",
    "        return (m.group(1) or \"\").strip()\n",
    "\n",
    "    unfenced = fence_re.sub(_strip_fences, s).strip()\n",
    "    m = re.search(r\"[\\{\\[]\", unfenced)\n",
    "    return unfenced[m.start():].strip() if m else \"\"\n",
    "\n",
    "\n",
    "def process_one_resume(jd: Dict[str, Any], resume_path: Path, structured_llm) -> Optional[Dict[str, Any]]:\n",
    "    text = load_resume_markdown(str(resume_path))\n",
    "    if not text.strip():\n",
    "        logging.warning(f\"Empty/unsupported resume: {resume_path.name}; skipping.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        r = call_llm_structured(structured_llm, jd, text)\n",
    "        # print(r.content)\n",
    "        cleaned_result = clean_text_v2(r.content)\n",
    "        # print(cleaned_result)\n",
    "        parsed = json.loads(cleaned_result)\n",
    "        # print(parsed)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"LLM error on {resume_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    candidate = safe_candidate_name_from_file(resume_path.name)\n",
    "    report = {\n",
    "        \"candidate_name\": candidate,\n",
    "        \"job_title\": jd.get(\"Job_Title\") or jd.get(\"job_title\"),\n",
    "        **parsed,\n",
    "    }\n",
    "    return report\n",
    "\n",
    "# ========== Batch processing ==========\n",
    "\n",
    "def process_all(job_description_file: str, resumes_folder: str, workers: int = 4, model: str = \"gpt-4o-mini\", topk: Optional[int] = None, topk_frac: float = 0.4) -> None:\n",
    "    reports_dir = Path(\"reports\"); reports_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    jd = load_yaml_job_description(job_description_file)\n",
    "    structured_llm = make_llm(model=model)\n",
    "\n",
    "    resume_files = [Path(resumes_folder) / fn for fn in os.listdir(resumes_folder)\n",
    "                    if fn.lower().endswith((\".pdf\", \".txt\"))]\n",
    "\n",
    "    reports: List[Dict[str, Any]] = []\n",
    "\n",
    "    # --- Load texts once for embedding + later scoring ---\n",
    "    resume_texts = [load_resume_markdown(str(p)) for p in resume_files]\n",
    "\n",
    "    # --- Pre-filter via embeddings ---\n",
    "    ranked = prefilter_resumes(jd, resume_files, resume_texts, topk=topk, topk_frac=topk_frac)\n",
    "    selected_files = [p for p, _ in ranked]\n",
    "    logging.info(f\"Pre-filter selected {len(selected_files)}/{len(resume_files)} resumes via embeddings\")\n",
    "\n",
    "    # Concurrency\n",
    "    with ThreadPoolExecutor(max_workers=max(1, int(workers))) as ex:\n",
    "        futs = {ex.submit(process_one_resume, jd, p, structured_llm): p for p in selected_files}\n",
    "        for fut in as_completed(futs):\n",
    "            p = futs[fut]\n",
    "            try:\n",
    "                rep = fut.result()\n",
    "                if rep:\n",
    "                    reports.append(rep)\n",
    "                    # write per-candidate JSON immediately\n",
    "                    out_path = Path(\"reports\") / f\"{rep['candidate_name']}_report.json\"\n",
    "                    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(rep, f, indent=2, ensure_ascii=False)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed {p.name}: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6cd7491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_description_file = r\"C:\\Users\\Lenovo\\resume_matcher\\jd.yaml\"\n",
    "# resumes_folder = r\"C:\\Users\\Lenovo\\resume_matcher\\resumes\"\n",
    "# topk = 2\n",
    "# Path(\"reports\").mkdir(exist_ok=True)\n",
    "\n",
    "# t0 = time.time()\n",
    "# reports = process_all(job_description_file=job_description_file, resumes_folder=resumes_folder, topk=topk)\n",
    "# logging.info(f\"Done processing resumes in {time.time() - t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de759704",
   "metadata": {},
   "source": [
    "# Langgraph agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7073dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === NEW / UPDATED SECTIONS BELOW ===\n",
    "# Add these imports\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ---------- Graph State ----------\n",
    "class ResumeState(TypedDict, total=False):\n",
    "    jd: Dict[str, Any]\n",
    "    resume_path: str\n",
    "    resume_text: str\n",
    "    llm: Any\n",
    "    raw_llm_output: str\n",
    "    parsed_json: Dict[str, Any]\n",
    "    report: Dict[str, Any]\n",
    "    errors: List[str]\n",
    "\n",
    "# ---------- Graph Nodes ----------\n",
    "def node_load_resume(state: ResumeState) -> ResumeState:\n",
    "    try:\n",
    "        text = load_resume_markdown(state[\"resume_path\"])\n",
    "        if not text.strip():\n",
    "            raise ValueError(\"Empty text extracted from resume\")\n",
    "        state[\"resume_text\"] = text\n",
    "    except Exception as e:\n",
    "        errs = state.get(\"errors\", [])\n",
    "        errs.append(f\"load_resume: {e}\")\n",
    "        state[\"errors\"] = errs\n",
    "        # You could choose to raise to stop the graph early\n",
    "        raise\n",
    "    return state\n",
    "\n",
    "# def node_call_llm(state: ResumeState) -> ResumeState:\n",
    "#     try:\n",
    "#         r = call_llm_structured(state[\"llm\"], state[\"jd\"], state[\"resume_text\"])\n",
    "#         print(r.content)\n",
    "#         state[\"raw_llm_output\"] = r.content\n",
    "#         cleaned = clean_text_v2(r.content)\n",
    "#         state[\"parsed_json\"] = json.loads(cleaned)\n",
    "#     except Exception as e:\n",
    "#         errs = state.get(\"errors\", [])\n",
    "#         errs.append(f\"call_llm: {e}\")\n",
    "#         state[\"errors\"] = errs\n",
    "#         raise\n",
    "#     return state\n",
    "from requests.exceptions import Timeout, RequestException\n",
    "import random\n",
    "def node_call_llm(state: ResumeState) -> ResumeState:\n",
    "    max_retries = 3\n",
    "    backoff_factor = 1  # seconds\n",
    "    timeout = 10  # seconds\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            # Attempt to call the LLM\n",
    "            r = call_llm_structured(state[\"llm\"], state[\"jd\"], state[\"resume_text\"])\n",
    "            print(r.content)\n",
    "            state[\"raw_llm_output\"] = r.content\n",
    "\n",
    "            # Process the response\n",
    "            cleaned = clean_text_v2(r.content)\n",
    "            state[\"parsed_json\"] = json.loads(cleaned)\n",
    "            return state\n",
    "\n",
    "        except Timeout as e:\n",
    "            # Handle timeout errors\n",
    "            error_message = f\"Attempt {attempt} failed due to timeout: {e}\"\n",
    "            print(error_message)\n",
    "            if attempt == max_retries:\n",
    "                state[\"errors\"] = state.get(\"errors\", []) + [error_message]\n",
    "                raise\n",
    "\n",
    "        except RequestException as e:\n",
    "            # Handle other request-related errors\n",
    "            error_message = f\"Attempt {attempt} failed due to request error: {e}\"\n",
    "            print(error_message)\n",
    "            if attempt == max_retries:\n",
    "                state[\"errors\"] = state.get(\"errors\", []) + [error_message]\n",
    "                raise\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle unexpected errors\n",
    "            error_message = f\"Attempt {attempt} failed due to unexpected error: {e}\"\n",
    "            print(error_message)\n",
    "            if attempt == max_retries:\n",
    "                state[\"errors\"] = state.get(\"errors\", []) + [error_message]\n",
    "                raise\n",
    "\n",
    "        # Calculate exponential backoff with jitter\n",
    "        sleep_time = backoff_factor * (2 ** (attempt - 1)) + random.uniform(0, 1)\n",
    "        print(f\"Retrying in {sleep_time:.2f} seconds...\")\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    # If all attempts fail, raise an exception\n",
    "    raise Exception(\"All retry attempts failed.\")\n",
    "\n",
    "def node_build_report(state: ResumeState) -> ResumeState:\n",
    "    try:\n",
    "        resume_path_str = state[\"resume_path\"]\n",
    "        jd = state[\"jd\"]\n",
    "        parsed = state[\"parsed_json\"]\n",
    "        candidate = safe_candidate_name_from_file(Path(resume_path_str).name)\n",
    "        report = {\n",
    "            \"candidate_name\": candidate,\n",
    "            \"job_title\": jd.get(\"Job_Title\") or jd.get(\"job_title\"),\n",
    "            **parsed,\n",
    "        }\n",
    "        state[\"report\"] = report\n",
    "    except Exception as e:\n",
    "        errs = state.get(\"errors\", [])\n",
    "        errs.append(f\"build_report: {e}\")\n",
    "        state[\"errors\"] = errs\n",
    "        raise\n",
    "    return state\n",
    "\n",
    "def node_save_report(state: ResumeState) -> ResumeState:\n",
    "    try:\n",
    "        reports_dir = Path(\"reports\"); reports_dir.mkdir(exist_ok=True)\n",
    "        rep = state[\"report\"]\n",
    "        out_path = reports_dir / f\"{rep['candidate_name']}_report.json\"\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(rep, f, indent=2, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        errs = state.get(\"errors\", [])\n",
    "        errs.append(f\"save_report: {e}\")\n",
    "        state[\"errors\"] = errs\n",
    "        raise\n",
    "    return state\n",
    "\n",
    "# ---------- Graph Builder ----------\n",
    "def build_resume_graph(structured_llm) -> Any:\n",
    "    \"\"\"\n",
    "    Build a LangGraph pipeline for SINGLE resume processing:\n",
    "      load_resume -> call_llm -> build_report -> save_report -> END\n",
    "    The compiled graph reuses your existing helpers & prompt.\n",
    "    \"\"\"\n",
    "    graph = StateGraph(ResumeState)\n",
    "    graph.add_node(\"load_resume\", node_load_resume)\n",
    "    graph.add_node(\"call_llm\", node_call_llm)\n",
    "    graph.add_node(\"build_report\", node_build_report)\n",
    "    graph.add_node(\"save_report\", node_save_report)\n",
    "\n",
    "    graph.set_entry_point(\"load_resume\")\n",
    "    graph.add_edge(\"load_resume\", \"call_llm\")\n",
    "    graph.add_edge(\"call_llm\", \"build_report\")\n",
    "    graph.add_edge(\"build_report\", \"save_report\")\n",
    "    graph.add_edge(\"save_report\", END)\n",
    "\n",
    "    # In-memory checkpointing is handy for debugging / retries\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "# ---------- Single-run Helper ----------\n",
    "def run_single_resume_with_graph(jd: Dict[str, Any], resume_path: Path, structured_llm) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Invoke the compiled LangGraph workflow for a single resume.\n",
    "    Returns the final report dict or None on failure.\n",
    "    \"\"\"\n",
    "    g = build_resume_graph(structured_llm)\n",
    "    init: ResumeState = {\n",
    "        \"jd\": jd,\n",
    "        \"resume_path\": str(resume_path),\n",
    "        \"llm\": structured_llm,\n",
    "        \"errors\": [],\n",
    "    }\n",
    "    try:\n",
    "        final_state = g.invoke(init)\n",
    "        return final_state.get(\"report\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Graph failed for {Path(resume_path).name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ---------- UPDATED process_all ----------\n",
    "def process_all(job_description_file: str, resumes_folder: str, workers: int = 2, model: str = \"gpt-4o-mini\", topk: Optional[int] = None, topk_frac: float = 0.4) -> None:\n",
    "    reports_dir = Path(\"reports\"); reports_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    jd = load_yaml_job_description(job_description_file)\n",
    "\n",
    "    # Gather candidate files\n",
    "    resume_files = [Path(resumes_folder) / fn for fn in os.listdir(resumes_folder)\n",
    "                    if fn.lower().endswith((\".pdf\", \".txt\"))]\n",
    "\n",
    "    # Pre-load resume texts once for embedding prefilter\n",
    "    resume_texts = [load_resume_markdown(str(p)) for p in resume_files]\n",
    "\n",
    "    # Pre-filter via embeddings\n",
    "    ranked = prefilter_resumes(jd, resume_files, resume_texts, topk=topk, topk_frac=topk_frac)\n",
    "    selected_files = [p for p, _ in ranked]\n",
    "    logging.info(f\"Pre-filter selected {len(selected_files)}/{len(resume_files)} resumes via embeddings\")\n",
    "\n",
    "    reports: List[Dict[str, Any]] = []\n",
    "\n",
    "    def _worker(resume_path: Path) -> Optional[Dict[str, Any]]:\n",
    "        # Safer to create an LLM client per thread\n",
    "        structured_llm = make_llm(model=model)\n",
    "        return run_single_resume_with_graph(jd, resume_path, structured_llm)\n",
    "\n",
    "    # Fan out across a thread pool, each calling the LangGraph workflow\n",
    "    with ThreadPoolExecutor(max_workers=max(1, int(workers))) as ex:\n",
    "        futs = {ex.submit(_worker, p): p for p in selected_files}\n",
    "        for fut in as_completed(futs):\n",
    "            p = futs[fut]\n",
    "            try:\n",
    "                rep = fut.result()\n",
    "                if rep:\n",
    "                    reports.append(rep)\n",
    "                    # (Already saved inside the graph's save node)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed {p.name}: {e}\")\n",
    "\n",
    "    logging.info(f\"Wrote {len(reports)} reports to {reports_dir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c01f7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_25780\\1506756277.py:269: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  _embedder = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "2025-09-20 21:09:34,217 INFO PyTorch version 2.7.1 available.\n",
      "2025-09-20 21:09:35,239 INFO Use pytorch device_name: cpu\n",
      "2025-09-20 21:09:35,241 INFO Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-09-20 21:09:40,770 INFO Pre-filter selected 5/5 resumes via embeddings\n",
      "2025-09-20 21:09:44,802 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-20 21:09:44,858 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "Attempt 1 failed due to unexpected error: Expecting value: line 1 column 1 (char 0)\n",
      "Retrying in 1.63 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 21:09:47,162 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:09:47,166 INFO Retrying request to /chat/completions in 0.472110 seconds\n",
      "2025-09-20 21:09:48,793 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:09:48,794 INFO Retrying request to /chat/completions in 0.894162 seconds\n",
      "2025-09-20 21:09:50,534 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:09:50,551 INFO Backing off call_llm_structured(...) for 0.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1758412800000'}, 'provider_name': None}}, 'user_id': 'user_32jjQAk4Yf8Ph3ZmokpzY4iHmXE'})\n",
      "2025-09-20 21:09:52,349 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:09:52,349 INFO Retrying request to /chat/completions in 0.458528 seconds\n",
      "2025-09-20 21:09:53,433 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:09:53,435 INFO Retrying request to /chat/completions in 0.864874 seconds\n",
      "2025-09-20 21:09:55,016 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:09:55,016 INFO Backing off call_llm_structured(...) for 0.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1758412800000'}, 'provider_name': None}}, 'user_id': 'user_32jjQAk4Yf8Ph3ZmokpzY4iHmXE'})\n",
      "2025-09-20 21:09:55,985 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:09:55,998 INFO Retrying request to /chat/completions in 0.447651 seconds\n",
      "2025-09-20 21:09:57,116 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:09:57,118 INFO Retrying request to /chat/completions in 0.805820 seconds\n",
      "2025-09-20 21:09:58,175 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:09:58,175 INFO Backing off call_llm_structured(...) for 3.1s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '16', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1758382800000'}, 'provider_name': None}}, 'user_id': 'user_32jjQAk4Yf8Ph3ZmokpzY4iHmXE'})\n",
      "2025-09-20 21:10:00,824 ERROR Graph failed for Resume_Farhan_Ali.pdf: 'list' object is not a mapping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"matched_required_skills\": [\"python\", \"rest apis\", \"sql\", \"version control (git)\", \"debugging and code review\"],\n",
      "  \"missing_required_skills\": [],\n",
      "  \"matched_optional_skills\": [\"docker\", \"aws lambda\", \"flask or fastapi\", \"ci/cd pipelines\"],\n",
      "  \"education_match\": \"true; B.Tech in Information Technology\",\n",
      "  \"experience_match\": \"true; 3.5 years of relevant backend development experience\",\n",
      "  \"keywords_matched\": [\"python\", \"rest apis\", \"sql\", \"version control (git)\", \"debugging and code review\", \"docker\", \"matched_optional_skills\": [\"docker\", \"aws lambda\", \"flask or fastapi\", \"ci/cd pipelines\"],\n",
      "  \"education_match\": \"true; B.Tech in Information Technology\",\n",
      "  \"experience_match\": \"true; 3.5 years of relevant backend development experience\",\n",
      "  \"keywords_matched\": [\"python\", \"rest apis\", \"sql\", \"version control (git)\", \"debugging and code review\", \"docker\", \"aws lambda\", \"flask\", \"fastapi\", \"ci/cd pipelines\"],\n",
      "  \"soft_skills_match\": [\"security\", \"monitoring\"],\n",
      "  \"resume_summary\": \"Farhan Ali has 3.5 years of backend development experience, focusing on fintech APIs and payment integrations, with strong skills in Python, Flask, and security practices.\",\n",
      "  \"match_score\": 0.85,\n",
      "  \"city_tier_match\": false,\n",
      "  \"longest_tenure_months\": 27,\n",
      "  \"final_score\": 82,\n",
      "  \"detected_city\": \"hyderabad\",\n",
      "  \"detected_city_tier\": 1,\n",
      "  \"city_tier_match\": false,\n",
      "  \"max_job_gap_months\": 1,\n",
      "  \"max_job_gap_months_check\": \"true; the maximum job gap is within the specified limit.\"\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 21:10:01,698 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:01,702 INFO Retrying request to /chat/completions in 0.476645 seconds\n",
      "2025-09-20 21:10:02,339 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:02,343 INFO Retrying request to /chat/completions in 0.814946 seconds\n",
      "2025-09-20 21:10:02,937 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:02,937 INFO Retrying request to /chat/completions in 0.385174 seconds\n",
      "2025-09-20 21:10:04,343 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:04,349 INFO Backing off call_llm_structured(...) for 1.7s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1758412800000'}, 'provider_name': None}}, 'user_id': 'user_32jjQAk4Yf8Ph3ZmokpzY4iHmXE'})\n",
      "2025-09-20 21:10:04,410 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:04,412 INFO Retrying request to /chat/completions in 0.920716 seconds\n",
      "2025-09-20 21:10:06,312 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:06,315 INFO Backing off call_llm_structured(...) for 0.2s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1758412800000'}, 'provider_name': None}}, 'user_id': 'user_32jjQAk4Yf8Ph3ZmokpzY4iHmXE'})\n",
      "2025-09-20 21:10:06,849 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:06,849 INFO Retrying request to /chat/completions in 0.462936 seconds\n",
      "2025-09-20 21:10:07,202 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:07,205 INFO Retrying request to /chat/completions in 0.428423 seconds\n",
      "2025-09-20 21:10:07,753 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:07,753 INFO Retrying request to /chat/completions in 0.804490 seconds\n",
      "2025-09-20 21:10:08,587 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:08,589 INFO Retrying request to /chat/completions in 0.781672 seconds\n",
      "2025-09-20 21:10:09,033 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:09,048 INFO Backing off call_llm_structured(...) for 4.3s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1758412800000'}, 'provider_name': None}}, 'user_id': 'user_32jjQAk4Yf8Ph3ZmokpzY4iHmXE'})\n",
      "2025-09-20 21:10:10,584 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:10,591 INFO Backing off call_llm_structured(...) for 1.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1758412800000'}, 'provider_name': None}}, 'user_id': 'user_32jjQAk4Yf8Ph3ZmokpzY4iHmXE'})\n",
      "2025-09-20 21:10:12,507 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:12,508 INFO Retrying request to /chat/completions in 0.398674 seconds\n",
      "2025-09-20 21:10:13,770 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:13,773 INFO Retrying request to /chat/completions in 0.792509 seconds\n",
      "2025-09-20 21:10:14,018 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:14,021 INFO Retrying request to /chat/completions in 0.464412 seconds\n",
      "2025-09-20 21:10:14,734 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:14,734 INFO Retrying request to /chat/completions in 0.988539 seconds\n",
      "2025-09-20 21:10:14,973 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:14,976 INFO Backing off call_llm_structured(...) for 2.9s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1758412800000'}, 'provider_name': None}}, 'user_id': 'user_32jjQAk4Yf8Ph3ZmokpzY4iHmXE'})\n",
      "2025-09-20 21:10:16,098 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:16,098 INFO Backing off call_llm_structured(...) for 22.6s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '20', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1758382860000'}, 'provider_name': None}}, 'user_id': 'user_32jjQAk4Yf8Ph3ZmokpzY4iHmXE'})\n",
      "2025-09-20 21:10:18,836 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:18,848 INFO Retrying request to /chat/completions in 0.383875 seconds\n",
      "2025-09-20 21:10:19,430 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:19,431 INFO Retrying request to /chat/completions in 0.860527 seconds\n",
      "2025-09-20 21:10:20,568 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:20,573 INFO Backing off call_llm_structured(...) for 1.5s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '16', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1758382860000'}, 'provider_name': None}}, 'user_id': 'user_32jjQAk4Yf8Ph3ZmokpzY4iHmXE'})\n",
      "2025-09-20 21:10:23,050 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:23,050 INFO Retrying request to /chat/completions in 0.477988 seconds\n",
      "2025-09-20 21:10:23,823 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:23,825 INFO Retrying request to /chat/completions in 0.983413 seconds\n",
      "2025-09-20 21:10:24,982 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:24,982 INFO Backing off call_llm_structured(...) for 14.4s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '16', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1758382860000'}, 'provider_name': None}}, 'user_id': 'user_32jjQAk4Yf8Ph3ZmokpzY4iHmXE'})\n",
      "2025-09-20 21:10:40,331 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:40,333 INFO Retrying request to /chat/completions in 0.481181 seconds\n",
      "2025-09-20 21:10:40,436 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:40,438 INFO Retrying request to /chat/completions in 0.487116 seconds\n",
      "2025-09-20 21:10:41,434 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:41,437 INFO Retrying request to /chat/completions in 0.897670 seconds\n",
      "2025-09-20 21:10:41,537 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:41,537 INFO Retrying request to /chat/completions in 0.761816 seconds\n",
      "2025-09-20 21:10:42,749 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:42,752 INFO Backing off call_llm_structured(...) for 37.8s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '16', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1758382860000'}, 'provider_name': None}}, 'user_id': 'user_32jjQAk4Yf8Ph3ZmokpzY4iHmXE'})\n",
      "2025-09-20 21:10:42,791 INFO HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-09-20 21:10:42,791 INFO Backing off call_llm_structured(...) for 18.0s (openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-min. ', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '16', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1758382860000'}, 'provider_name': None}}, 'user_id': 'user_32jjQAk4Yf8Ph3ZmokpzY4iHmXE'})\n"
     ]
    }
   ],
   "source": [
    "job_description_file = r\"C:\\Users\\Lenovo\\resume_matcher\\jd.yaml\"\n",
    "resumes_folder = r\"C:\\Users\\Lenovo\\resume_matcher\\resumes\"\n",
    "topk = 5\n",
    "Path(\"reports\").mkdir(exist_ok=True)\n",
    "\n",
    "t0 = time.time()\n",
    "reports = process_all(job_description_file=job_description_file, resumes_folder=resumes_folder, topk=topk)\n",
    "logging.info(f\"Done processing resumes in {time.time() - t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757325d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
